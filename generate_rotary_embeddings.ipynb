{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import notebook\n",
    "import tqdm, tqdm.notebook\n",
    "tqdm.tqdm = tqdm.notebook.tqdm  # notebook-friendly progress bars\n",
    "from tqdm import tqdm\n",
    "from hloc.utils.read_write_model import read_images_binary,read_points3D_binary\n",
    "from PIL import Image\n",
    "from scipy.interpolate import griddata\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import notebook\n",
    "import tqdm, tqdm.notebook\n",
    "tqdm.tqdm = tqdm.notebook.tqdm  # notebook-friendly progress bars\n",
    "from tqdm import tqdm\n",
    "from hloc.utils.read_write_model import read_images_binary,read_points3D_binary\n",
    "from PIL import Image\n",
    "from scipy.interpolate import griddata\n",
    "from tqdm import tqdm\n",
    "from hloc.utils.read_write_model import read_images_binary,read_points3D_binary,read_cameras_binary,qvec2rotmat\n",
    "from PIL import Image\n",
    "from scipy.interpolate import griddata\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path='outputs/aachen/sfm_superpoint+superglue/images.bin'\n",
    "points3Ds_path='outputs/aachen/sfm_superpoint+superglue/points3D.bin'\n",
    "caperas_path='outputs/aachen/sfm_superpoint+superglue/cameras.bin'\n",
    "images = read_images_binary(images_path)\n",
    "points3Ds = read_points3D_binary(points3Ds_path)\n",
    "cameras = read_cameras_binary(caperas_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"segment-anything-main/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patch_2D_points(xys, img_width, img_height, size):\n",
    "    patch_width = img_width // size  # 16x16 patches to get 256 patches\n",
    "    patch_height = img_height // size\n",
    "\n",
    "    patches_2D_points = {}\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            # Define patch boundaries\n",
    "            x_start, x_end = i * patch_width, (i + 1) * patch_width\n",
    "            y_start, y_end = j * patch_height, (j + 1) * patch_height\n",
    "            \n",
    "            # Find 2D points inside the patch\n",
    "            mask = (xys[:, 0] >= x_start) & (xys[:, 0] < x_end) & (xys[:, 1] >= y_start) & (xys[:, 1] < y_end)\n",
    "            \n",
    "            # Get the 2D points for the points inside the patch\n",
    "            patch_2D_points = xys[mask]\n",
    "            \n",
    "            # Store in the dictionary\n",
    "            patches_2D_points[(j, i)] = patch_2D_points.tolist()\n",
    "\n",
    "    return patches_2D_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_img_id=[1167,3266,3302,3297]\n",
    "# For each image in the modified images dictionary\n",
    "iter=0\n",
    "small=7\n",
    "large =4\n",
    "Pre_Small= 20\n",
    "Pre_Large = 14\n",
    "size=Pre_Large\n",
    "TotalImageKey = {}\n",
    "TotalImageKey3D = {}\n",
    "iter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b870af80194ba1bb03c0294b71421e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/cvnar/Spatial_Contrastive_Loss/4_Rotary_Positional_Embedding_Real.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar/Spatial_Contrastive_Loss/4_Rotary_Positional_Embedding_Real.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(TopSky)\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar/Spatial_Contrastive_Loss/4_Rotary_Positional_Embedding_Real.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(image, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cvnar/Spatial_Contrastive_Loss/4_Rotary_Positional_Embedding_Real.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     predictor\u001b[39m.\u001b[39;49mset_image(image)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar/Spatial_Contrastive_Loss/4_Rotary_Positional_Embedding_Real.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     input_points_AllBuilding\u001b[39m=\u001b[39m[]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cvnar/Spatial_Contrastive_Loss/4_Rotary_Positional_Embedding_Real.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     patch_info\u001b[39m=\u001b[39m{}\n",
      "File \u001b[0;32m~/anaconda3/envs/JKK/lib/python3.8/site-packages/segment_anything/predictor.py:60\u001b[0m, in \u001b[0;36mSamPredictor.set_image\u001b[0;34m(self, image, image_format)\u001b[0m\n\u001b[1;32m     57\u001b[0m input_image_torch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(input_image, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     58\u001b[0m input_image_torch \u001b[39m=\u001b[39m input_image_torch\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()[\u001b[39mNone\u001b[39;00m, :, :, :]\n\u001b[0;32m---> 60\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_torch_image(input_image_torch, image\u001b[39m.\u001b[39;49mshape[:\u001b[39m2\u001b[39;49m])\n",
      "File \u001b[0;32m~/anaconda3/envs/JKK/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/JKK/lib/python3.8/site-packages/segment_anything/predictor.py:89\u001b[0m, in \u001b[0;36mSamPredictor.set_torch_image\u001b[0;34m(self, transformed_image, original_image_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(transformed_image\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:])\n\u001b[1;32m     88\u001b[0m input_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpreprocess(transformed_image)\n\u001b[0;32m---> 89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mimage_encoder(input_image)\n\u001b[1;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_image_set \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/JKK/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/JKK/lib/python3.8/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[39m=\u001b[39m blk(x)\n\u001b[1;32m    114\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneck(x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/JKK/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/JKK/lib/python3.8/site-packages/segment_anything/modeling/image_encoder.py:174\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     H, W \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n\u001b[1;32m    172\u001b[0m     x, pad_hw \u001b[39m=\u001b[39m window_partition(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size)\n\u001b[0;32m--> 174\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(x)\n\u001b[1;32m    175\u001b[0m \u001b[39m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/JKK/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/JKK/lib/python3.8/site-packages/segment_anything/modeling/image_encoder.py:234\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m attn \u001b[39m=\u001b[39m (q \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale) \u001b[39m@\u001b[39m k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 234\u001b[0m     attn \u001b[39m=\u001b[39m add_decomposed_rel_pos(attn, q, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrel_pos_h, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrel_pos_w, (H, W), (H, W))\n\u001b[1;32m    236\u001b[0m attn \u001b[39m=\u001b[39m attn\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    237\u001b[0m x \u001b[39m=\u001b[39m (attn \u001b[39m@\u001b[39m v)\u001b[39m.\u001b[39mview(B, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, H, W, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\u001b[39m.\u001b[39mreshape(B, H, W, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/JKK/lib/python3.8/site-packages/segment_anything/modeling/image_encoder.py:349\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    347\u001b[0m q_h, q_w \u001b[39m=\u001b[39m q_size\n\u001b[1;32m    348\u001b[0m k_h, k_w \u001b[39m=\u001b[39m k_size\n\u001b[0;32m--> 349\u001b[0m Rh \u001b[39m=\u001b[39m get_rel_pos(q_h, k_h, rel_pos_h)\n\u001b[1;32m    350\u001b[0m Rw \u001b[39m=\u001b[39m get_rel_pos(q_w, k_w, rel_pos_w)\n\u001b[1;32m    352\u001b[0m B, _, dim \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/envs/JKK/lib/python3.8/site-packages/segment_anything/modeling/image_encoder.py:322\u001b[0m, in \u001b[0;36mget_rel_pos\u001b[0;34m(q_size, k_size, rel_pos)\u001b[0m\n\u001b[1;32m    319\u001b[0m k_coords \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(k_size)[\u001b[39mNone\u001b[39;00m, :] \u001b[39m*\u001b[39m \u001b[39mmax\u001b[39m(q_size \u001b[39m/\u001b[39m k_size, \u001b[39m1.0\u001b[39m)\n\u001b[1;32m    320\u001b[0m relative_coords \u001b[39m=\u001b[39m (q_coords \u001b[39m-\u001b[39m k_coords) \u001b[39m+\u001b[39m (k_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mmax\u001b[39m(q_size \u001b[39m/\u001b[39m k_size, \u001b[39m1.0\u001b[39m)\n\u001b[0;32m--> 322\u001b[0m \u001b[39mreturn\u001b[39;00m rel_pos_resized[relative_coords\u001b[39m.\u001b[39;49mlong()]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for image_id, image in tqdm(images.items()):\n",
    "    xys = image.xys\n",
    "    point3D_ids = image.point3D_ids\n",
    "\n",
    "    # Get the image dimensions\n",
    "    img = Image.open('datasets/aachen/images/images_upright/'+image.name)\n",
    "    img_width, img_height = img.size\n",
    "\n",
    "    # Extract 3D ids for each patch in the image\n",
    "    patches_2D_ids = extract_patch_2D_points(xys, img_width, img_height, size)\n",
    "    patches_2D_ids=dict(list(patches_2D_ids.items())) \n",
    "    ########3\n",
    "    key = image_id\n",
    "    tar_img = images[key]\n",
    "    tar_cameras = cameras[key]\n",
    "    tar_camera_header = [tar_cameras.id, tar_cameras.width, tar_cameras.height, *tar_cameras.params]\n",
    "    tar_image_header = [tar_img.id, *tar_img.qvec, *tar_img.tvec, tar_img.camera_id, tar_img.name]\n",
    "    tar_points_strings = []\n",
    "\n",
    "    # for xy, point3D_id in zip(tar_img.xys, tar_img.point3D_ids):\n",
    "    #     if(point3D_id == -1):\n",
    "    #         continue\n",
    "    #     tar_points_strings.append(\" \".join(map(str, [*xy, point3D_id])))\n",
    "\n",
    "\n",
    "\n",
    "    image_dir =Path('datasets/aachen/images/images_upright/')\n",
    "    path = image_dir / tar_image_header[9]\n",
    "    image = cv2.imread(str(path))\n",
    "\n",
    "\n",
    "    # #for str_points in tar_points_strings:\n",
    "    TopSky=[]\n",
    "    ElseKey=[]\n",
    "    for key, patches in patches_2D_ids.items():\n",
    "        if not patches:            \n",
    "            # Compute the center of the patch\n",
    "            #print(key)\n",
    "            patch_center_x = key[1] * (img_width // size) + (img_width // (2*size))\n",
    "            patch_center_y = key[0] * (img_height // size) + (img_height // (2*size))\n",
    "            if key[0]==0:\n",
    "                TopSky.append([patch_center_x,patch_center_y])\n",
    "        ##여기에 추가적으로 아무 값이 없는것은 lable을 추가해 주어야 한다. \n",
    "    ImageKey = {}\n",
    "    if len(TopSky)==0:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        predictor.set_image(image)\n",
    "        input_points_AllBuilding=[]\n",
    "        patch_info={}\n",
    "        for key, patches in patches_2D_ids.items():\n",
    "            x_start, x_end = key[1] * (img_width // size), (key[1] + 1) * (img_width // size)\n",
    "            y_start, y_end = key[0] * (img_height // size), (key[0] + 1) * (img_height // size)\n",
    "\n",
    "            patch_center_x = key[1] * (img_width // size) + (img_width // (2*size))\n",
    "            patch_center_y = key[0] * (img_height // size) + (img_height // (2*size))\n",
    "            \n",
    "            patch_info[key] = {\n",
    "                \"x_start\": x_start, \"x_end\": x_end,\n",
    "                \"y_start\": y_start, \"y_end\": y_end,\n",
    "                \"center\": [patch_center_x, patch_center_y]\n",
    "            }\n",
    "            patch_data = patch_info[key]\n",
    "            input_points_AllBuilding.append(patch_data[\"center\"])\n",
    "            ImageKey[key]=patch_data[\"center\"]\n",
    "        \n",
    "        input_points_AllBuilding = np.array(input_points_AllBuilding)\n",
    "        input_labels_AllBuilding = np.zeros(len(input_points_AllBuilding),dtype=int)\n",
    "\n",
    "        #plt.figure(figsize=(5,5))\n",
    "        #plt.imshow(image)\n",
    "        #show_points(input_points_AllBuilding, input_labels_AllBuilding, plt.gca())\n",
    "        #plt.axis('off')\n",
    "        #filename = \"NoMask.png\"\n",
    "        #imgname=str(tar_image_header[9]).split('.')[0].split('/')[-1]\n",
    "        #filename = \"Segmentation/MultiMaskAll4x4_09292035/\"+imgname+\"_\"+filename\n",
    "        #plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "    else:\n",
    "        #하늘이 없는 이미지에도 패치를 그려야 한다. \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        predictor.set_image(image)\n",
    "\n",
    "        input_points_TopSky = np.array(TopSky)\n",
    "        input_labels_TopSky = np.ones(len(TopSky),dtype=int)\n",
    "\n",
    "        masks, scores, logits = predictor.predict(\n",
    "        point_coords=input_points_TopSky,\n",
    "        point_labels=input_labels_TopSky,\n",
    "        multimask_output=True,\n",
    "        )\n",
    "        imgname=str(tar_image_header[9]).split('.')[0].split('/')[-1]\n",
    "        for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "            if i == 1:\n",
    "                patch_info={}\n",
    "                input_points_YesSky=[]\n",
    "                input_points_NoSky=[]\n",
    "                for key, patches in patches_2D_ids.items():\n",
    "                    x_start, x_end = key[1] * (img_width // size), (key[1] + 1) * (img_width // size)\n",
    "                    y_start, y_end = key[0] * (img_height // size), (key[0] + 1) * (img_height // size)\n",
    "\n",
    "                    patch_center_x = key[1] * (img_width // size) + (img_width // (2*size))\n",
    "                    patch_center_y = key[0] * (img_height // size) + (img_height // (2*size))\n",
    "                    \n",
    "                    patch_info[key] = {\n",
    "                        \"x_start\": x_start, \"x_end\": x_end,\n",
    "                        \"y_start\": y_start, \"y_end\": y_end,\n",
    "                        \"center\": [patch_center_x, patch_center_y]\n",
    "                    }\n",
    "                    patch_data = patch_info[key]\n",
    "                    patch_mask = masks[0, patch_data[\"y_start\"]:patch_data[\"y_end\"], patch_data[\"x_start\"]:patch_data[\"x_end\"]]\n",
    "                    \n",
    "            # 패치 내의 픽셀 중 90% 이상이 True인지 검사\n",
    "                    true_percentage = np.sum(patch_mask) / (patch_mask.shape[0] * patch_mask.shape[1])\n",
    "                    \n",
    "                    if true_percentage >= 0.9:\n",
    "                        input_points_YesSky.append(patch_data[\"center\"])\n",
    "                        ImageKey[key]=[0,0]\n",
    "                    else:\n",
    "                        input_points_NoSky.append(patch_data[\"center\"])\n",
    "                        ImageKey[key]=patch_data[\"center\"]\n",
    "\n",
    "                input_points_YesSky = np.array(input_points_YesSky)\n",
    "                input_labels_YesSky = np.ones(len(input_points_YesSky),dtype=int)\n",
    "\n",
    "                input_points_NoSky = np.array(input_points_NoSky)\n",
    "                input_labels_NoSky = np.zeros(len(input_points_NoSky),dtype=int)\n",
    "                if len(input_labels_YesSky) > 0:\n",
    "                    input_points = np.vstack((input_points_YesSky, input_points_NoSky))\n",
    "                    input_labels = np.concatenate((input_labels_YesSky, input_labels_NoSky))\n",
    "\n",
    "\n",
    "\n",
    "                #plt.figure(figsize=(5,5))\n",
    "                #plt.imshow(image)\n",
    "                #show_mask(mask, plt.gca())\n",
    "                #show_points(input_points, input_labels, plt.gca())\n",
    "                #plt.axis('off')\n",
    "                #filename = f\"mask_{i+1}_score_{score:.3f}.png\"\n",
    "                #filename = \"Segmentation/MultiMaskAll4x4_09292035/\"+imgname+\"_\"+filename\n",
    "                #plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "    ImageKey = {key: ImageKey[key] for key in sorted(ImageKey.keys(), key=lambda x: (x[0], x[1]))}\n",
    "    TotalImageKey[image_id]=ImageKey\n",
    "    #3D Point 변환\n",
    "    tar_quat = np.array(tar_image_header[1:5])\n",
    "    rvec=qvec2rotmat(tar_quat)\n",
    "    tar_trans = np.array(tar_image_header[5:8])\n",
    "    camera_metrix = np.zeros((3, 3), dtype='float32')\n",
    "    camera_metrix[0, 0] = tar_camera_header[3]\n",
    "    camera_metrix[1, 1] = tar_camera_header[3]\n",
    "    camera_metrix[0, 2] = tar_camera_header[4]\n",
    "    camera_metrix[1, 2] = tar_camera_header[5]\n",
    "    camera_metrix[2, 2] = 1\n",
    "    distort=tar_camera_header[6]\n",
    "    ImageKey3D={}\n",
    "    ImageKey3D['Token'] = images[image_id].tvec\n",
    "    for key, patches in TotalImageKey[image_id].items():\n",
    "        if patches ==[0,0]:\n",
    "            #ImageKey3D[key]=[0,0,0]\n",
    "            #ImageKey3D[key]=np.zeros(3)\n",
    "            ImageKey3D[key]=np.array([100000,100000,100000])\n",
    "        else:\n",
    "            target_img = images[image_id]\n",
    "            target_cam = cameras[target_img.camera_id]\n",
    "            K = np.array([\n",
    "                [target_cam.params[0], 0, target_cam.params[1]],\n",
    "                [0, target_cam.params[0], target_cam.params[2]],\n",
    "                [0, 0, 1]\n",
    "            ])\n",
    "            R = qvec2rotmat(target_img.qvec)\n",
    "            t = target_img.tvec.reshape(3, 1)\n",
    "            # RT = np.hstack([R, t])\n",
    "            RT_4x4 = np.eye(4)\n",
    "            RT_4x4[:3, :3] = R\n",
    "            RT_4x4[:3, 3] = t.reshape(-1)\n",
    "            RT=RT_4x4\n",
    "            point_2d = np.array([int(patches[0]), int(patches[1]), 1]).reshape(3, 1)\n",
    "            point_3d_cam = np.linalg.inv(K).dot(point_2d)\n",
    "            point_3d_cam = point_3d_cam / np.linalg.norm(point_3d_cam)  # Normalize to keep it at unit distance\n",
    "\n",
    "            # Transforming the 3D point in camera coordinate system to world coordinate system\n",
    "            point_3d_world = np.linalg.inv(RT).dot(np.vstack([point_3d_cam * 10, 1]))  # Multiplying by 10 to keep the depth arbitrary\n",
    "            point_3d_world = point_3d_world[:3].reshape(-1)\n",
    "            ImageKey3D[key]=point_3d_world\n",
    "    TotalImageKey3D[image_id]=ImageKey3D\n",
    "    iter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DataBase/TotalImageKey3D_14x14.pickle\", \"wb\") as file:\n",
    "    pickle.dump(TotalImageKey3D, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load pickle file\n",
    "with open('DataBase/TotalImageKey3D_14x14.pickle', 'rb') as f:\n",
    "    all_representative_points = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dc6d7c9f4e459985773a49fd4e4706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for key in tqdm(images.keys()):\n",
    "    # 각 키에 대응하는 3D 점을 가져옴\n",
    "    points = all_representative_points[key]\n",
    "\n",
    "    # images[key].qvec 값을 가져옴\n",
    "    qvec = torch.tensor(images[key].qvec)\n",
    "\n",
    "    # 사전의 각 점에 대해 qvec 추가\n",
    "    for point_key in points:\n",
    "        # 현재 점을 PyTorch 텐서로 변환\n",
    "        current_point = torch.tensor(points[point_key])\n",
    "\n",
    "        # 현재 점과 qvec 연결\n",
    "        concatenated_point = torch.cat((current_point, qvec), dim=0)\n",
    "\n",
    "        # 업데이트된 값을 사전에 저장\n",
    "        points[point_key] = concatenated_point\n",
    "\n",
    "    # 업데이트된 사전을 원본 사전에 저장\n",
    "    all_representative_points[key] = points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DataBase/TotalImageKeyRT_14x14.pickle', 'wb') as f:\n",
    "    pickle.dump(all_representative_points, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.5375e+02, -1.2629e+02, -5.7944e+02,  5.0160e-02,  5.5191e-01,\n",
       "         9.5616e-02,  8.2689e-01], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_representative_points[630]['Token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def rotary_position_embeddings_7d(coords, dim):\n",
    "    # Normalize coordinates to [-1, 1]\n",
    "    # Normalize coordinates to [-1, 1]\n",
    "    coords = 2.0 * (coords - coords.min()) / (coords.max() - coords.min()) - 1.0\n",
    "\n",
    "    # Convert normalized coordinates to angles in range [-pi, pi]\n",
    "    angles = coords * torch.pi\n",
    "\n",
    "    # Compute sin and cos for each dimension\n",
    "    sin = torch.sin(angles)\n",
    "    cos = torch.cos(angles)\n",
    "\n",
    "    # Create embeddings by interleaving sin and cos values\n",
    "    embeddings = torch.zeros(*coords.shape[:-1], dim)\n",
    "\n",
    "    # Calculate the number of complete sin-cos pairs that can fit in the embeddings\n",
    "    full_pairs = dim // 14\n",
    "\n",
    "    for i in range(full_pairs):\n",
    "        embeddings[..., 14*i:14*i+7] = sin\n",
    "        embeddings[..., 14*i+7:14*i+14] = cos\n",
    "\n",
    "    # Handle the case where dim is not a multiple of 14\n",
    "    remaining_dims = dim % 14\n",
    "    if remaining_dims > 0:\n",
    "        # Calculate the repeat count for each dimension\n",
    "        sin_repeat_count = [1] * (sin.dim() - 1) + [(remaining_dims + 6) // 7]\n",
    "        cos_repeat_count = [1] * (cos.dim() - 1) + [(remaining_dims + 6) // 7]\n",
    "\n",
    "        # Repeat sin and cos to fill the remaining dimensions\n",
    "        extended_sin = sin.repeat(*sin_repeat_count)\n",
    "        extended_cos = cos.repeat(*cos_repeat_count)\n",
    "\n",
    "        if remaining_dims <= 7:\n",
    "            embeddings[..., -remaining_dims:] = extended_sin[..., :remaining_dims]\n",
    "        else:\n",
    "            embeddings[..., -remaining_dims:-7] = extended_sin[..., :remaining_dims-7]\n",
    "            embeddings[..., -7:] = extended_cos[..., :(remaining_dims-7)]\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def rotary_position_embeddings_7d_to_192d(coords):\n",
    "    \"\"\"\n",
    "    Compute Rotary Position Embeddings for 7D coordinates, extending them to 192 dimensions.\n",
    "\n",
    "    Args:\n",
    "    coords (Tensor): A tensor of shape (..., 7) representing 7D coordinates.\n",
    "\n",
    "    Returns:\n",
    "    Tensor: A tensor of rotary position embeddings with 192 dimensions.\n",
    "    \"\"\"\n",
    "    dim = 192\n",
    "    # Normalize coordinates to [-1, 1]\n",
    "    coords = 2.0 * (coords - coords.min()) / (coords.max() - coords.min()) - 1.0\n",
    "\n",
    "    # Convert normalized coordinates to angles in range [-pi, pi]\n",
    "    angles = coords * torch.pi\n",
    "\n",
    "    # Compute sin and cos for each dimension\n",
    "    sin = torch.sin(angles)\n",
    "    cos = torch.cos(angles)\n",
    "\n",
    "    # Initialize the embeddings tensor\n",
    "    embeddings = torch.zeros(*coords.shape[:-1], dim)\n",
    "\n",
    "    # Determine the number of complete 14-dimensional (7 sin + 7 cos) blocks\n",
    "    full_blocks = dim // 14\n",
    "\n",
    "    # Fill the embeddings tensor with repeated sin and cos blocks\n",
    "    for i in range(full_blocks):\n",
    "        embeddings[..., 14*i:14*i+7] = sin\n",
    "        embeddings[..., 14*i+7:14*i+14] = cos\n",
    "\n",
    "    # Handle remaining dimensions if dim is not a multiple of 14\n",
    "    remaining_dims = dim % 14\n",
    "    if remaining_dims > 0:\n",
    "        extended_sin_cos = torch.cat((sin, cos), dim=-1).flatten()\n",
    "        embeddings[..., -remaining_dims:] = extended_sin_cos[:remaining_dims]\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086b592ac0154644982388733f7521a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38912/3292865358.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(value)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dim = 192\n",
    "include_input=True\n",
    "encoded_points = {}\n",
    "include_input = True\n",
    "for id, reps in tqdm(all_representative_points.items()):\n",
    "    encoded_points[id] = {}\n",
    "    for key, value in reps.items():\n",
    "        if value is not None:\n",
    "            input = torch.tensor(value)\n",
    "            input = input.unsqueeze(0)\n",
    "            output = rotary_position_embeddings_7d_to_192d(input)\n",
    "            encoded_points[id][key] = torch.tensor(output.squeeze(0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe3d25fa5f34cc38e502e174dec6b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38912/1490317307.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  token_tensor = torch.tensor(token_value)\n",
      "/tmp/ipykernel_38912/1490317307.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  patch_tensors = [torch.tensor(patch_value) for patch_value in patch_values]\n"
     ]
    }
   ],
   "source": [
    "tensor_dict = {}\n",
    "for key in tqdm(encoded_points):\n",
    "    token_value = encoded_points[key]['Token']\n",
    "    patch_values = [encoded_points[key][patch_key] for patch_key in encoded_points[key] if patch_key != 'Token']\n",
    "\n",
    "    token_tensor = torch.tensor(token_value)\n",
    "    patch_tensors = [torch.tensor(patch_value) for patch_value in patch_values]\n",
    "\n",
    "    # torch.stack을 사용하여 텐서들을 쌓습니다.\n",
    "    tensor_array = torch.stack([token_tensor] + patch_tensors)\n",
    "\n",
    "    tensor_dict[key] = tensor_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DataBase/Large_Patch_14x14_RT_RoPE_Tensor.pickle', 'wb') as f:\n",
    "    pickle.dump(tensor_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기까지"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JKK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
