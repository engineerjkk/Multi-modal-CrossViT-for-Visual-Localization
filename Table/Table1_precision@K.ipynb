{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm, tqdm.notebook\n",
    "tqdm.tqdm = tqdm.notebook.tqdm\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from hloc.utils.read_write_model import read_images_binary\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "# Base path configuration (상대 경로)\n",
    "BASE_PATH = '..'\n",
    "\n",
    "# Derived paths\n",
    "PATHS = {\n",
    "    'model_base': os.path.join(BASE_PATH, 'model'),\n",
    "    'database': os.path.join(BASE_PATH, 'DataBase'),\n",
    "    'dataset': os.path.join(BASE_PATH, 'datasets'),\n",
    "    'global_desc': os.path.join(BASE_PATH, 'GlobalDescriptors'),\n",
    "    'fold_dataset': os.path.join(BASE_PATH, '5FoldDataset'),\n",
    "    'outputs': os.path.join(BASE_PATH, 'outputs')\n",
    "}\n",
    "\n",
    "TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "K_VALUES = [200, 150, 100, 50, 20, 5, 1]\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    '1st': {'path': os.path.join(PATHS['model_base'], 'B_1_1st_Train_Student/best_model8929_0.6792626728110606.pth')},\n",
    "    '2nd': {'path': os.path.join(PATHS['model_base'], 'B_1_2nd_Train_Student/best_model9827_0.6837962962962971.pth')},\n",
    "    '3rd': {'path': os.path.join(PATHS['model_base'], 'B_1_3rd_Train_Student/best_model9887_0.6800925925925929.pth')},\n",
    "    '4th': {'path': os.path.join(PATHS['model_base'], 'B_1_4th_Train_Student/best_model9244_0.6696759259259274.pth')},\n",
    "    '5th': {'path': os.path.join(PATHS['model_base'], 'B_1_5th_Train_Student/best_model9397_0.6780092592592604.pth')}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm, tqdm.notebook\n",
    "tqdm.tqdm = tqdm.notebook.tqdm\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from hloc.utils.read_write_model import read_images_binary\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Base path configuration (상대 경로)\n",
    "BASE_PATH = '..'\n",
    "\n",
    "# Derived paths\n",
    "PATHS = {\n",
    "    'model_base': os.path.join(BASE_PATH, 'model'),\n",
    "    'database': os.path.join(BASE_PATH, 'DataBase'),\n",
    "    'dataset': os.path.join(BASE_PATH, 'datasets'),\n",
    "    'global_desc': os.path.join(BASE_PATH, 'GlobalDescriptors'),\n",
    "    'fold_dataset': os.path.join(BASE_PATH, '5FoldDataset'),\n",
    "    'outputs': os.path.join(BASE_PATH, 'outputs')\n",
    "}\n",
    "\n",
    "TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "K_VALUES = [200, 150, 100, 50, 20, 5, 1]\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    '1st': {'path': os.path.join(PATHS['model_base'], 'B_1_1st_Train_Student/best_model8929_0.6792626728110606.pth')},\n",
    "    '2nd': {'path': os.path.join(PATHS['model_base'], 'B_1_2nd_Train_Student/best_model9827_0.6837962962962971.pth')},\n",
    "    '3rd': {'path': os.path.join(PATHS['model_base'], 'B_1_3rd_Train_Student/best_model9887_0.6800925925925929.pth')},\n",
    "    '4th': {'path': os.path.join(PATHS['model_base'], 'B_1_4th_Train_Student/best_model9244_0.6696759259259274.pth')},\n",
    "    '5th': {'path': os.path.join(PATHS['model_base'], 'B_1_5th_Train_Student/best_model9397_0.6780092592592604.pth')}\n",
    "}\n",
    "\n",
    "class ImageDataset:\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(PATHS['dataset'], 'aachen/images/images_upright', self.data_frame['Anchor'][idx])\n",
    "        anchor_img = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            anchor_img = self.transform(anchor_img)\n",
    "        return anchor_img\n",
    "\n",
    "def load_model(model_path):\n",
    "    from models.crossvit_official import crossvit_tiny_224\n",
    "    model = crossvit_tiny_224(pretrained=True)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    new_state_dict = {k[7:] if 'module.' in k else k: v \n",
    "                     for k, v in checkpoint['crossvit_state_dict'].items()}\n",
    "    model.load_state_dict(new_state_dict, strict=True)\n",
    "    model = model.cuda()\n",
    "    model = nn.DataParallel(model, device_ids=[0,1])\n",
    "    return model\n",
    "\n",
    "def extract_features(model, dataset_path, output_size):\n",
    "    dataset = ImageDataset(csv_file=dataset_path, transform=TRANSFORM)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "    features = torch.zeros(len(dataset), output_size).cuda()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, anchor in enumerate(dataloader):\n",
    "            anchor = anchor.cuda(non_blocking=True)\n",
    "            image_encoding = model(anchor)\n",
    "            image_encoding = F.normalize(image_encoding, p=2, dim=1)\n",
    "            features[i] = image_encoding\n",
    "            \n",
    "    return features.cpu().numpy()\n",
    "\n",
    "def calculate_neighbors(train_features, query_features, k):\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(train_features)\n",
    "    distances, indices = nbrs.kneighbors(query_features)\n",
    "    return distances[:, 1:], indices[:, 1:]\n",
    "\n",
    "def save_references(indices, query_table, database_table, output_path):\n",
    "    with open(output_path, \"w\") as file:\n",
    "        for i in range(len(indices)):\n",
    "            for index in indices[i]:\n",
    "                file.writelines(f\"{query_table['Anchor'][i]} {database_table['Anchor'][index]}\\n\")\n",
    "    \n",
    "    # Sort lines\n",
    "    with open(output_path) as f:\n",
    "        lines = sorted(f.readlines())\n",
    "    with open(output_path, 'w') as file:\n",
    "        for line in lines:\n",
    "            file.writelines(line)\n",
    "\n",
    "def calculate_accuracy(reference_file, database_path, images_path):\n",
    "    images = read_images_binary(images_path)\n",
    "    with open(database_path, 'rb') as f:\n",
    "        anc_pos_relations = pickle.load(f)\n",
    "        \n",
    "    search_results = {}\n",
    "    total_image_id = list(images.keys())\n",
    "    \n",
    "    with open(reference_file, 'r') as file:\n",
    "        for line in file:\n",
    "            anchor_img, result_img = line.strip().split()\n",
    "            anchor_key = None\n",
    "            result_key = None\n",
    "            for j in total_image_id:\n",
    "                tar_img = images[j]\n",
    "                if tar_img.name == anchor_img:\n",
    "                    anchor_key = j\n",
    "                if tar_img.name == result_img:\n",
    "                    result_key = j\n",
    "            if anchor_key not in search_results:\n",
    "                search_results[anchor_key] = []\n",
    "            search_results[anchor_key].append(result_key)\n",
    "            \n",
    "    total_positive_rates = []\n",
    "    for anchor_key, results in search_results.items():\n",
    "        positive_count = 0\n",
    "        anchor_positives = []\n",
    "        for item in anc_pos_relations:\n",
    "            if item['Anchor'][0] == anchor_key:\n",
    "                anchor_positives = item['Positive']\n",
    "                break\n",
    "                \n",
    "        for result_key in results:\n",
    "            if result_key in anchor_positives:\n",
    "                positive_count += 1\n",
    "                \n",
    "        denominator = min(len(results), len(anchor_positives))\n",
    "        positive_rate = positive_count / denominator if denominator > 0 else 0\n",
    "        total_positive_rates.append(positive_rate)\n",
    "        \n",
    "    return np.mean(total_positive_rates)\n",
    "\n",
    "def process_model(model_name):\n",
    "    print(f\"\\nProcessing {model_name} model...\")\n",
    "    \n",
    "    model = load_model(MODEL_CONFIGS[model_name]['path'])\n",
    "    \n",
    "    print(\"Extracting features...\")\n",
    "    database_path = os.path.join(PATHS['database'], 'DataBase4328.csv')\n",
    "    query_path = os.path.join(PATHS['fold_dataset'], f'anchor_student_{model_name}.csv')\n",
    "    \n",
    "    train_features = extract_features(model, database_path, 1000)\n",
    "    query_features = extract_features(model, query_path, 1000)\n",
    "    \n",
    "    results = {}\n",
    "    for k in K_VALUES:\n",
    "        distances, indices = calculate_neighbors(train_features, query_features, k)\n",
    "        output_path = os.path.join(PATHS['global_desc'], f'{model_name}_StudentReference_{k}.txt')\n",
    "        \n",
    "        pd_database = pd.read_csv(database_path)\n",
    "        query_table = pd.read_csv(query_path)\n",
    "        \n",
    "        save_references(indices, query_table, pd_database, output_path)\n",
    "        accuracy = calculate_accuracy(\n",
    "            output_path,\n",
    "            os.path.join(PATHS['database'], 'DataBase_Norm_Tiny.pickle'),\n",
    "            os.path.join(PATHS['outputs'], 'aachen/sfm_superpoint+superglue/images.bin')\n",
    "        )\n",
    "        \n",
    "        results[k] = accuracy\n",
    "        print(f\"k={k}: {accuracy:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_all_models():\n",
    "    all_results = {}\n",
    "    for model_name in tqdm(MODEL_CONFIGS.keys()):\n",
    "        results = process_model(model_name)\n",
    "        all_results[model_name] = results\n",
    "    return all_results\n",
    "\n",
    "def display_results(all_results):\n",
    "    print(\"\\nFinal Results Summary:\")\n",
    "    \n",
    "    # Print header\n",
    "    print(\"\\nk values:\", end=\"\")\n",
    "    for k in K_VALUES:\n",
    "        print(f\"\\t{k}\", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    # Print results for each model\n",
    "    for model_name in sorted(all_results.keys()):\n",
    "        print(f\"\\n{model_name} model:\", end=\"\")\n",
    "        for k in K_VALUES:\n",
    "            print(f\"\\t{all_results[model_name][k]:.4f}\", end=\"\")\n",
    "    \n",
    "    print(\"\\n\\n=== Average Values for Each k ===\")\n",
    "    # Calculate and print average for each k separately\n",
    "    for k in K_VALUES:\n",
    "        values = [results[k] for results in all_results.values()]\n",
    "        avg = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        max_val = np.max(values)\n",
    "        min_val = np.min(values)\n",
    "        max_model = max(all_results.items(), key=lambda x: x[1][k])[0]\n",
    "        min_model = min(all_results.items(), key=lambda x: x[1][k])[0]\n",
    "        \n",
    "        print(f\"\\nFor k = {k}:\")\n",
    "        print(f\"Average: {avg:.4f}\")\n",
    "        print(f\"Std Dev: {std:.4f}\")\n",
    "        print(f\"Max: {max_val:.4f} (Model: {max_model})\")\n",
    "        print(f\"Min: {min_val:.4f} (Model: {min_model})\")\n",
    "        \n",
    "        # Individual model values\n",
    "        print(\"Individual model values:\")\n",
    "        for model_name in sorted(all_results.keys()):\n",
    "            print(f\"  {model_name} model: {all_results[model_name][k]:.4f}\")\n",
    "\n",
    "    # Summary print for just the averages\n",
    "    print(\"\\n=== Summary of Averages ===\")\n",
    "    print(\"print('Average values for different k:')\")\n",
    "    for k in K_VALUES:\n",
    "        avg = np.mean([results[k] for results in all_results.values()])\n",
    "        print(f\"k = {k}: {avg:.4f}\")\n",
    "\n",
    "# Optional: Save detailed results\n",
    "def save_detailed_results(all_results):\n",
    "    detailed_results = {\n",
    "        'individual_results': all_results,\n",
    "        'statistics': {}\n",
    "    }\n",
    "    \n",
    "    for k in K_VALUES:\n",
    "        values = [results[k] for results in all_results.values()]\n",
    "        detailed_results['statistics'][k] = {\n",
    "            'average': float(np.mean(values)),\n",
    "            'std_dev': float(np.std(values)),\n",
    "            'max': {\n",
    "                'value': float(np.max(values)),\n",
    "                'model': max(all_results.items(), key=lambda x: x[1][k])[0]\n",
    "            },\n",
    "            'min': {\n",
    "                'value': float(np.min(values)),\n",
    "                'model': min(all_results.items(), key=lambda x: x[1][k])[0]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    with open('precision@K_results.json', 'w') as f:\n",
    "        json.dump(detailed_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76c86fb82794c9d8180395551ed2421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 1st model...\n",
      "Extracting features...\n",
      "k=200: 0.8196\n",
      "k=150: 0.7710\n",
      "k=100: 0.7177\n",
      "k=50: 0.6851\n",
      "k=20: 0.7290\n",
      "k=5: 0.8363\n",
      "k=1: 0.9021\n",
      "\n",
      "Processing 2nd model...\n",
      "Extracting features...\n",
      "k=200: 0.8305\n",
      "k=150: 0.7825\n",
      "k=100: 0.7303\n",
      "k=50: 0.6984\n",
      "k=20: 0.7476\n",
      "k=5: 0.8432\n",
      "k=1: 0.8924\n",
      "\n",
      "Processing 3rd model...\n",
      "Extracting features...\n",
      "k=200: 0.8303\n",
      "k=150: 0.7809\n",
      "k=100: 0.7259\n",
      "k=50: 0.6871\n",
      "k=20: 0.7368\n",
      "k=5: 0.8426\n",
      "k=1: 0.9062\n",
      "\n",
      "Processing 4th model...\n",
      "Extracting features...\n",
      "k=200: 0.8157\n",
      "k=150: 0.7677\n",
      "k=100: 0.7149\n",
      "k=50: 0.6847\n",
      "k=20: 0.7320\n",
      "k=5: 0.8232\n",
      "k=1: 0.8843\n",
      "\n",
      "Processing 5th model...\n",
      "Extracting features...\n",
      "k=200: 0.8081\n",
      "k=150: 0.7615\n",
      "k=100: 0.7147\n",
      "k=50: 0.6922\n",
      "k=20: 0.7475\n",
      "k=5: 0.8406\n",
      "k=1: 0.9016\n"
     ]
    }
   ],
   "source": [
    "all_results = process_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results Summary:\n",
      "\n",
      "k values:\t200\t150\t100\t50\t20\t5\t1\n",
      "\n",
      "1st model:\t0.8196\t0.7710\t0.7177\t0.6851\t0.7290\t0.8363\t0.9021\n",
      "2nd model:\t0.8305\t0.7825\t0.7303\t0.6984\t0.7476\t0.8432\t0.8924\n",
      "3rd model:\t0.8303\t0.7809\t0.7259\t0.6871\t0.7368\t0.8426\t0.9062\n",
      "4th model:\t0.8157\t0.7677\t0.7149\t0.6847\t0.7320\t0.8232\t0.8843\n",
      "5th model:\t0.8081\t0.7615\t0.7147\t0.6922\t0.7475\t0.8406\t0.9016\n",
      "\n",
      "=== Average Values for Each k ===\n",
      "\n",
      "For k = 200:\n",
      "Average: 0.8208\n",
      "Std Dev: 0.0086\n",
      "Max: 0.8305 (Model: 2nd)\n",
      "Min: 0.8081 (Model: 5th)\n",
      "Individual model values:\n",
      "  1st model: 0.8196\n",
      "  2nd model: 0.8305\n",
      "  3rd model: 0.8303\n",
      "  4th model: 0.8157\n",
      "  5th model: 0.8081\n",
      "\n",
      "For k = 150:\n",
      "Average: 0.7727\n",
      "Std Dev: 0.0080\n",
      "Max: 0.7825 (Model: 2nd)\n",
      "Min: 0.7615 (Model: 5th)\n",
      "Individual model values:\n",
      "  1st model: 0.7710\n",
      "  2nd model: 0.7825\n",
      "  3rd model: 0.7809\n",
      "  4th model: 0.7677\n",
      "  5th model: 0.7615\n",
      "\n",
      "For k = 100:\n",
      "Average: 0.7207\n",
      "Std Dev: 0.0063\n",
      "Max: 0.7303 (Model: 2nd)\n",
      "Min: 0.7147 (Model: 5th)\n",
      "Individual model values:\n",
      "  1st model: 0.7177\n",
      "  2nd model: 0.7303\n",
      "  3rd model: 0.7259\n",
      "  4th model: 0.7149\n",
      "  5th model: 0.7147\n",
      "\n",
      "For k = 50:\n",
      "Average: 0.6895\n",
      "Std Dev: 0.0052\n",
      "Max: 0.6984 (Model: 2nd)\n",
      "Min: 0.6847 (Model: 4th)\n",
      "Individual model values:\n",
      "  1st model: 0.6851\n",
      "  2nd model: 0.6984\n",
      "  3rd model: 0.6871\n",
      "  4th model: 0.6847\n",
      "  5th model: 0.6922\n",
      "\n",
      "For k = 20:\n",
      "Average: 0.7386\n",
      "Std Dev: 0.0077\n",
      "Max: 0.7476 (Model: 2nd)\n",
      "Min: 0.7290 (Model: 1st)\n",
      "Individual model values:\n",
      "  1st model: 0.7290\n",
      "  2nd model: 0.7476\n",
      "  3rd model: 0.7368\n",
      "  4th model: 0.7320\n",
      "  5th model: 0.7475\n",
      "\n",
      "For k = 5:\n",
      "Average: 0.8372\n",
      "Std Dev: 0.0074\n",
      "Max: 0.8432 (Model: 2nd)\n",
      "Min: 0.8232 (Model: 4th)\n",
      "Individual model values:\n",
      "  1st model: 0.8363\n",
      "  2nd model: 0.8432\n",
      "  3rd model: 0.8426\n",
      "  4th model: 0.8232\n",
      "  5th model: 0.8406\n",
      "\n",
      "For k = 1:\n",
      "Average: 0.8973\n",
      "Std Dev: 0.0079\n",
      "Max: 0.9062 (Model: 3rd)\n",
      "Min: 0.8843 (Model: 4th)\n",
      "Individual model values:\n",
      "  1st model: 0.9021\n",
      "  2nd model: 0.8924\n",
      "  3rd model: 0.9062\n",
      "  4th model: 0.8843\n",
      "  5th model: 0.9016\n",
      "\n",
      "=== Summary of Averages ===\n",
      "print('Average values for different k:')\n",
      "k = 200: 0.8208\n",
      "k = 150: 0.7727\n",
      "k = 100: 0.7207\n",
      "k = 50: 0.6895\n",
      "k = 20: 0.7386\n",
      "k = 5: 0.8372\n",
      "k = 1: 0.8973\n"
     ]
    }
   ],
   "source": [
    "display_results(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JKK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
