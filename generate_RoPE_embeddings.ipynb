{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from hloc.utils.read_write_model import read_images_binary, read_points3D_binary, read_cameras_binary, qvec2rotmat\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# Configure tqdm for notebook\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = 'outputs/aachen/sfm_superpoint+superglue/images.bin'\n",
    "POINTS3D_PATH = 'outputs/aachen/sfm_superpoint+superglue/points3D.bin'\n",
    "CAMERAS_PATH = 'outputs/aachen/sfm_superpoint+superglue/cameras.bin'\n",
    "SAM_CHECKPOINT = \"segment-anything-main/sam_vit_h_4b8939.pth\"\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "OUTPUT_PATH = Path('DataBase')\n",
    "PATCH_SIZE = 14  # Grid size for patches\n",
    "EMBEDDING_DIM = 192  # Dimension for rotary embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models_and_data():\n",
    "    \"\"\"Initialize SAM model and load necessary data\"\"\"\n",
    "    # Load COLMAP data\n",
    "    images = read_images_binary(IMAGES_PATH)\n",
    "    points3Ds = read_points3D_binary(POINTS3D_PATH)\n",
    "    cameras = read_cameras_binary(CAMERAS_PATH)\n",
    "    \n",
    "    # Initialize SAM\n",
    "    device = \"cuda\"\n",
    "    sam = sam_model_registry[MODEL_TYPE](checkpoint=SAM_CHECKPOINT)\n",
    "    sam.to(device=device)\n",
    "    predictor = SamPredictor(sam)\n",
    "    \n",
    "    return images, points3Ds, cameras, predictor\n",
    "\n",
    "images, points3Ds, cameras, predictor = initialize_models_and_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patch_2D_points(xys, img_width, img_height, size):\n",
    "    \"\"\"Extract 2D points for each patch in the image grid\"\"\"\n",
    "    patch_width = img_width // size\n",
    "    patch_height = img_height // size\n",
    "    \n",
    "    patches_2D_points = {}\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            x_start, x_end = i * patch_width, (i + 1) * patch_width\n",
    "            y_start, y_end = j * patch_height, (j + 1) * patch_height\n",
    "            \n",
    "            mask = (xys[:, 0] >= x_start) & (xys[:, 0] < x_end) & \\\n",
    "                   (xys[:, 1] >= y_start) & (xys[:, 1] < y_end)\n",
    "            \n",
    "            patches_2D_points[(j, i)] = xys[mask].tolist()\n",
    "    \n",
    "    return dict(sorted(patches_2D_points.items()))\n",
    "\n",
    "def convert_2D_to_3D(point_2d, K, R, t):\n",
    "    \"\"\"Convert 2D point to 3D world coordinates\"\"\"\n",
    "    RT_4x4 = np.eye(4)\n",
    "    RT_4x4[:3, :3] = R\n",
    "    RT_4x4[:3, 3] = t.reshape(-1)\n",
    "    \n",
    "    point_2d = np.array([int(point_2d[0]), int(point_2d[1]), 1]).reshape(3, 1)\n",
    "    point_3d_cam = np.linalg.inv(K).dot(point_2d)\n",
    "    point_3d_cam = point_3d_cam / np.linalg.norm(point_3d_cam)\n",
    "    \n",
    "    point_3d_world = np.linalg.inv(RT_4x4).dot(np.vstack([point_3d_cam * 10, 1]))\n",
    "    return point_3d_world[:3].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9470fff989094287bcbf77a6edae580b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images:   0%|          | 0/4328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_image(image_id, image, predictor):\n",
    "    \"\"\"Process single image and extract patch information\"\"\"\n",
    "    xys = image.xys\n",
    "    img = Image.open(f'datasets/aachen/images/images_upright/{image.name}')\n",
    "    img_width, img_height = img.size\n",
    "    \n",
    "    # Extract patch information\n",
    "    patches_2D = extract_patch_2D_points(xys, img_width, img_height, PATCH_SIZE)\n",
    "    \n",
    "    # Load and process image with SAM\n",
    "    img_cv = cv2.imread(f'datasets/aachen/images/images_upright/{image.name}')\n",
    "    img_rgb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
    "    predictor.set_image(img_rgb)\n",
    "    \n",
    "    # Process patches\n",
    "    patch_info = {}\n",
    "    for key, patches in patches_2D.items():\n",
    "        x_start = key[1] * (img_width // PATCH_SIZE)\n",
    "        y_start = key[0] * (img_height // PATCH_SIZE)\n",
    "        center_x = x_start + (img_width // (2 * PATCH_SIZE))\n",
    "        center_y = y_start + (img_height // (2 * PATCH_SIZE))\n",
    "        \n",
    "        patch_info[key] = [center_x, center_y]\n",
    "    \n",
    "    return patch_info\n",
    "\n",
    "TotalImageKey = {}\n",
    "for image_id, image in tqdm(images.items(), desc=\"Processing images\"):\n",
    "    TotalImageKey[image_id] = process_image(image_id, image, predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_patches_to_3D(image_id, patches, images, cameras):\n",
    "    \"\"\"Convert patch centers to 3D coordinates\"\"\"\n",
    "    image = images[image_id]\n",
    "    camera = cameras[image.camera_id]\n",
    "    \n",
    "    # Camera parameters\n",
    "    K = np.array([\n",
    "        [camera.params[0], 0, camera.params[1]],\n",
    "        [0, camera.params[0], camera.params[2]],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    R = qvec2rotmat(image.qvec)\n",
    "    t = image.tvec\n",
    "    \n",
    "    patches_3D = {'Token': torch.tensor(np.concatenate([t, image.qvec]))}\n",
    "    \n",
    "    for key, point_2d in patches.items():\n",
    "        if point_2d == [0, 0]:\n",
    "            patches_3D[key] = torch.tensor([100000, 100000, 100000, *image.qvec])\n",
    "        else:\n",
    "            point_3d = convert_2D_to_3D(point_2d, K, R, t)\n",
    "            patches_3D[key] = torch.tensor(np.concatenate([point_3d, image.qvec]))\n",
    "    \n",
    "    return patches_3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6492100071945398b1dd846182947cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting to 3D:   0%|          | 0/4328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TotalImageKey3D = {}\n",
    "for image_id in tqdm(TotalImageKey.keys(), desc=\"Converting to 3D\"):\n",
    "    TotalImageKey3D[image_id] = convert_patches_to_3D(\n",
    "        image_id, TotalImageKey[image_id], images, cameras)\n",
    "\n",
    "# Save intermediate results\n",
    "with open(OUTPUT_PATH / \"TotalImageKey3D_14x14.pickle\", \"wb\") as f:\n",
    "    pickle.dump(TotalImageKey3D, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotary_position_embeddings_7d_to_192d(coords):\n",
    "    \"\"\"Generate rotary position embeddings from 7D to 192D\"\"\"\n",
    "    # Normalize coordinates to [-1, 1]\n",
    "    coords = 2.0 * (coords - coords.min()) / (coords.max() - coords.min()) - 1.0\n",
    "    angles = coords * torch.pi\n",
    "    \n",
    "    # Compute sin and cos\n",
    "    sin = torch.sin(angles)\n",
    "    cos = torch.cos(angles)\n",
    "    \n",
    "    # Create embeddings\n",
    "    embeddings = torch.zeros(*coords.shape[:-1], EMBEDDING_DIM)\n",
    "    full_blocks = EMBEDDING_DIM // 14\n",
    "    \n",
    "    for i in range(full_blocks):\n",
    "        embeddings[..., 14*i:14*i+7] = sin\n",
    "        embeddings[..., 14*i+7:14*i+14] = cos\n",
    "    \n",
    "    # Handle remaining dimensions\n",
    "    remaining_dims = EMBEDDING_DIM % 14\n",
    "    if remaining_dims > 0:\n",
    "        extended_sin_cos = torch.cat((sin, cos), dim=-1).flatten()\n",
    "        embeddings[..., -remaining_dims:] = extended_sin_cos[:remaining_dims]\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc560bdbd0eb4521a192c3bbf8738906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/4328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate embeddings for all points\n",
    "encoded_points = {}\n",
    "for id, reps in tqdm(TotalImageKey3D.items(), desc=\"Generating embeddings\"):\n",
    "    encoded_points[id] = {}\n",
    "    for key, value in reps.items():\n",
    "        if value is not None:\n",
    "            input_tensor = value.unsqueeze(0)\n",
    "            output = rotary_position_embeddings_7d_to_192d(input_tensor)\n",
    "            encoded_points[id][key] = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeff40ce12154676b71178af4e4d9e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating final tensors:   0%|          | 0/4328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensor_dict = {}\n",
    "for key in tqdm(encoded_points, desc=\"Creating final tensors\"):\n",
    "    token_tensor = encoded_points[key]['Token']\n",
    "    patch_tensors = [encoded_points[key][k] for k in encoded_points[key] if k != 'Token']\n",
    "    tensor_dict[key] = torch.stack([token_tensor] + patch_tensors)\n",
    "\n",
    "# Save results\n",
    "with open(OUTPUT_PATH / 'Large_Patch_14x14_RT_RoPE_Tensor.pickle', 'wb') as f:\n",
    "    pickle.dump(tensor_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JKK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
